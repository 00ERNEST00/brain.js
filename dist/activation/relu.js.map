{"version":3,"sources":["../../src/activation/relu.js"],"names":["relu","reluDerivative","weight","Math","max","error"],"mappings":";;;;;QAMgBA,I,GAAAA,I;QAUAC,c,GAAAA,c;AAhBhB;;;;;;AAMO,SAASD,IAAT,CAAcE,MAAd,EAAsB;AAC3B,SAAOC,KAAKC,GAAL,CAAS,CAAT,EAAYF,MAAZ,CAAP;AACD;;AAED;;;;;;AAMO,SAASD,cAAT,CAAwBC,MAAxB,EAAgCG,KAAhC,EAAuC;AAC5C,SAAOH,SAAS,CAAT,GAAaG,KAAb,GAAqB,CAA5B;AACD","file":"relu.js","sourcesContent":["/**\n * Relu Activation, aka Rectified Linear Unit Activation\n * @description https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n * @param weight\n * @returns {number}\n */\nexport function relu(weight) {\n  return Math.max(0, weight);\n}\n\n/**\n * Leaky Relu derivative\n * @param weight\n * @param error\n * @returns {number}\n */\nexport function reluDerivative(weight, error) {\n  return weight > 0 ? error : 0;\n}"]}